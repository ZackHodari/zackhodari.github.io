<!DOCTYPE html>
<html lang="en">
	<!-- Template created by Pippa Shoemark -->

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <head>
        <!-- Global Site Tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106538118-1"></script>
        <!-- <script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script> -->
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments)};
            gtag('js', new Date());

             tgtag('config', 'UA-106538118-1');
        </script>

        <meta charset="utf-8">
        <title>Zack Hodari | Homepage</title>
        <link rel="stylesheet" type="text/css" href="style.css">
    </head>

    <body>

        <header>
            <div class = "wrapper">
                <div class="image-cropper">
                    <img src="headshot.jpeg" class="rounded">
                </div>
                <h1>Zack Hodari</h1>
                <nav>
                    <ul>
                        <!-- <li><a class="btn" id="current" href="#"> Home </a></li> -->
                        <li style="list-style-type:none;"><a class="btn" href="http://github.com/ZackHodari"> GitHub </a></li>
                        <li style="list-style-type:none;"><a class="btn" href="http://linkedin.com/in/zackhodari"> LinkedIn </a></li>
                        <li style="list-style-type:none;"><a class="btn" href="https://twitter.com/zhodari"> Twitter </a></li>
                        <li style="list-style-type:none;"><a class="btn" href="documents/CV.pdf"> CV.pdf </a></li>
                        <!-- <li><a class="fake_btn"> zack.hodari [at] ed.ac.uk </a></li> -->
                    </ul>
                </nav>
            </div>
        </header>


        <div class="wrapper">

            <main>

                <section id="about_me">
                    <h2> About Me </h2>
                    <p>
                        I am a researcher with 8 years of experience in generative AI, currently working on speech-to-speech translation at Papercup. Our mission is make the world's videos watchable in any language! I'm focused on delivering real-world impact through my research. My background is in speech synthesis and have experience in NLP, LLMs, and computer vision.
                    </p>
                    <!-- <p>
                        I am a PhD candidate at the <a href="http://datascience.inf.ed.ac.uk">Centre for Doctoral Training in Data Science</a> within Informatics at the University of Edinburgh. I am also affiliated with the <a href="http://www.cstr.ed.ac.uk">Centre for Speech Technology Research</a> and am supervised by <a href="http://homepages.inf.ed.ac.uk/simonk">Simon King</a>.
                    </p> -->
                </section>


                <section id="research">
                    <h2> Research </h2>
                    <p>
                        My research focuses on making synthetic voices speak like humans, this means conveying the same intentions, feelings, and attitudes to communicate effectively. These aspects of how we speak are called prosody. During my PhD at the University of Edinburgh, I focused on how we lack the contextual information required to make appropriate prosodic choices. My more recent work focuses on using additional context to generate appropriate prosody. I work on generative models, controllable machine learning, representation learning, interpretability, and experimental design.
                    </p>
                </section>


                <section id="blog">
                    <h2> Blog Posts </h2>

                    <p id="neurips_blog_2023">
                        <a class="btn" href="https://rich-tibia-0e6.notion.site/Great-ideas-at-NeurIPS-2023-f5e2bfdb53474e09a81b1d8664307408">Blog</a> Highlights of <cite>NeurIPS</cite>, New Orleans 2023.
                    </p>

                    <p id="interspeech_blog_2_2022">
                        <a class="btn" href="https://engineering.papercup.com/posts/interspeech2022-highlights/">Blog</a> Favourite papers at <cite>Interspeech</cite>, Seoul 2022.
                    </p>

                    <p id="interspeech_blog_1_2022">
                        <a class="btn" href="https://engineering.papercup.com/posts/interspeech2022-tts-overview/">Blog</a> Speech synthesis at <cite>Interspeech</cite>, Seoul 2022.
                    </p>

                    <p id="uk_speech_blog_2022">
                        <a class="btn" href="https://engineering.papercup.com/posts/uk-speech-2022/">Blog</a> Summary of <cite>UK Speech conference</cite>, Edinburgh 2022.
                    </p>

                    <p id="SLT_blog_2022">
                        <a class="btn" href="https://engineering.papercup.com/posts/sheffield-CDT-conf-2022/">Blog</a> Summary of <cite>Speech and Language Technology workshop</cite>, Sheffield 2022.
                    </p>

                    <p id="MLSLP_lectures_2021">
                        <a class="btn" href="https://drive.google.com/drive/folders/1j1sSoXcRILZtVbcdHyWdu_KOrM4kMqa6?usp=drive_link">slides.drive</a> 11 part <b>Lecture Series</b> on <cite>Machine Learning for Speech and Language Processing</cite> masters course. I designed the syllabus for this, created all lecture content, and gave the lecture series for students in 2021.
                    </p>

                    <p id="PMR_notes">
                        <a class="btn" href="https://drive.google.com/drive/folders/1KcQV4GiOeXzhLyVN29jVtbSabRsJL4hO?usp=drive_link">slides.drive</a> <b>Lecture notes</b> for <cite>Probabilistic Modelling and Reasoning</cite> (<a href="http://www.inf.ed.ac.uk/teaching/courses/pmr">PMR</a>) masters course. As the teaching assistant I created course notes to supplement the course material.
                    </p>

                    <p id="convolutions_tutorial">
                        <a class="btn" href="https://docs.google.com/presentation/d/1kL9jEF1ERMrRh4He1d6MjbOo-xCauNhJ9zYSrJ0bK5w/edit?usp=sharing">Tutorial</a> From linear regression to RNNs and CNNs, including animated explainer for all CNN variants. Tutorial was aimed at <cite>Speech and Language Processing</cite> masters students at The University of Edinburgh.
                    </p>
                </section>


                <section id="talks">
                    <h2>Highlighted Talks</h2>
                    <p>
                        I have a passion for presenting my ideas to others, including both technical and lay audiences. I've given many invited talks, including keynote presentations.
                    </p>
                    <ul>
                        <li id="SSSW_keynote_2023">
                            <b>Keynote talk</b> at Sheffield Speech Synthesis Workshop (2023): <cite>Prosody - The only important problem in TTS.</cite></br>
                            <a class="btn" href="https://docs.google.com/presentation/d/1byIDoiTRzsI2hDz5oX-ZfZVtF5HWWo78z8isLUK3xfw/edit?usp=sharing">slides.google</a>
                        </li>

                        <li id="3MT_talk_2021">
                            <b>University finalist</b> in 3 Minute Thesis (2021): <cite>How to speak like a human.</cite></br>
                            <a class="btn" href="https://www.ed.ac.uk/institute-academic-development/postgraduate/doctoral/3mt/history/2021-3mt-finalists-videos#uoe-sma-136097">video.html</a>
                        </li>

                        <li id="SIGML_talk_2021">
                            Invited talk at <a href="https://isca-speech.org/Machine-Learning-SIGML">ISCA SIGML</a> seminar series (2021) on my PhD research: <cite>Synthesising prosody with insufficient context.</cite></br>
                            <a class="btn" href="https://docs.google.com/presentation/d/1ddVv64zkFuGgk8o0nE6FqvnKK2DOZ4E7PCuAlYGPWpk/edit?usp=drive_link">slides.google</a>
                        </li>

                        <li id="KTH_talk_2021">
                            Invited talk at <a href="https://www.kth.se/is/tmh">KTH TMH</a> seminar series (2021) on my PhD research: <cite>Synthesising prosody with insufficient context.</cite></br>
                            <a class="btn" href="https://docs.google.com/presentation/d/1XMdlcRvkUBiAywG-8tsOlLoC9qWWU1Gtz6kMQNc0gg0/edit?usp=drive_link">slides.google</a>
                        </li>

                        <li id="Papercup_talk_2020">
                            Invited talk at <a href="https://www.papercup.com">Papercup</a> (2020): <cite>Perception of learned intonation contour classes.</cite></br>
                            <a class="btn" href="https://docs.google.com/presentation/d/1jtn-BlkaaJ9VTpNnEJ7xTIQbbNsVS9QXqubkVOeC59A/edit?usp=sharing">slides.google</a>
                        </li>

                    </ul>
                </section>


                <section id="publications">
                    <h2> Publications </h2>
                    <ul>
                        <li id="icassp_2024_sparse">
                            Dan Andrei Iliescu, Devang Savita Ram Mohan, Tian Huey Teh, <b>Zack Hodari</b>, (2024) <cite>Controllable Prosody Generation With Partial Inputs</cite> In Proceedings of ICASSP, Seoul, Korea, 2024.</br>
                            <a class="btn" href="https://arxiv.org/abs/2303.09446">arxiv.html</a>
                            <a class="btn" href="https://arxiv.org/pdf/2303.09446.pdf">paper.pdf</a>
                            <a class="btn" href="documents/ICASSP24_poster.pdf">poster.pdf</a>
                            <a class="btn" href="https://anonymous-submission-563098.github.io/sparse-control">demo.html</a>
                        </li>

                        <li id="icassp_2023_ensemble">
                            Tian Huey Teh, Vivian Hu, Devang Ram Mohan, <b>Zack Hodari</b>, Christopher Wallis, Tomás Gómez Ibarrondo, Alexandra Torresquintero, James Leoni, Mark Gales, Simon King, (2023) <cite>Ensemble Prosody Prediction For Expressive Speech Synthesis</cite> In Proceedings of ICASSP, Rhodes, Greece, 2023, pp. 1-5.</br>
                            <a class="btn" href="https://arxiv.org/abs/2304.00714">arxiv.html</a>
                            <a class="btn" href="https://arxiv.org/pdf/2304.00714.pdf">paper.pdf</a>
                        </li>

                        <li id="phd_research">
                            <b>Zack Hodari</b> (2017) <cite>Synthesising Prosody with Insufficient Context</cite>. PhD thesis, University of Edinburgh.</br>
                            <b>Summary: </b>Generating speech is difficult as the intonation, rhythm, emotion, and expressivity of our voice is implicit and not defined by the text. I worked on generative models, LLMs, discrete representation learning, interpretability, and controllability to improve the expressivity of synthetic voices.</br>
                            <a class="btn" href="http://dx.doi.org/10.7488/era/2654">abstract.html</a>
                            <a class="btn" href="documents/PhD_thesis_electronic.pdf">thesis.pdf</a>
                            <a class="btn" href="https://docs.google.com/presentation/d/1yCYIvjGFSnhECv4aYQUccLV0N2mCL-8nw3BSScubong/edit?usp=drive_link">slides.google</a>
                        </li>

                        <li id="icassp_2021_CAMP">
                            <b>Zack Hodari</b>, Alexis Moinet, Sri Karlapati, Jaime Lorenzo-Trueba, Thomas Merritt, Arnaud Joly, Ammar Abbas, Penny Karanasou, Thomas Drugman, (2021) <cite>CAMP: a two-stage approach to modelling prosody in context.</cite> In Proceedings of ICASSP, Toronto, Canada, 2021, pp. 6578-6582.</br>
                            <a class="btn" href="https://arxiv.org/abs/2011.01175">arxiv.html</a>
                            <a class="btn" href="https://arxiv.org/pdf/2011.01175.pdf">paper.pdf</a>
                            <a class="btn" href="documents/ICASSP21_slides.pptx">slides.pptx</a>
                        </li>

                        <li id="icassp_2021_kathaka">
                            Sri Karlapati, Ammar Abbas, <b>Zack Hodari</b>, Alexis Moinet, Arnaud Joly, Penny Karanasou, Thomas Drugman, (2021) <cite>Prosodic Representation Learning and Contextual Sampling for Neural Text-to-Speech.</cite> In Proceedings of ICASSP, Toronto, Canada, 2021, pp. 6573-6577.</br>
                            <a class="btn" href="https://arxiv.org/abs/2011.02252">arxiv.html</a>
                            <a class="btn" href="https://arxiv.org/pdf/2011.02252.pdf">paper.pdf</a>
                        </li>

                        <li id="sp_2020">
                            <b>Zack Hodari</b>, Catherine Lai, Simon King, (2020) <cite>Perception of prosodic variation for speech synthesis using an unsupervised discrete representation of F0.</cite> In Proceedings of Speech Prosody, Tokyo, Japan, 2020, pp. 965-969.</br>
                            <a class="btn" href="https://www.isca-speech.org/archive/SpeechProsody_2020/abstracts/235.html">bib.html</a>
                            <a class="btn" href="https://www.isca-speech.org/archive/SpeechProsody_2020/pdfs/235.pdf">paper.pdf</a>
                            <a class="btn" href="https://docs.google.com/presentation/d/1jtn-BlkaaJ9VTpNnEJ7xTIQbbNsVS9QXqubkVOeC59A/edit?usp=sharing">slides.google</a>
                            <a class="btn" href="documents/SP20_video.mp4">video.mp4</a>
                        </li>

                        <li id="ssw_2019">
                            <b>Zack Hodari</b>, Oliver Watts, Simon King, (2019) <cite>Using generative modelling to produce varied intonation for speech synthesis.</cite> In Proceedings of Speech Synthesis Workshop, Vienna, Austria, 2019, pp. 239-244.</br>
                            This work was also presented at UK Speech, Birmingham, UK, 2019.</br>
                            <a class="btn" href="https://www.isca-speech.org/archive/SSW_2019/abstracts/SSW10_P_3-3.html">bib.html</a>
                            <a class="btn" href="https://www.isca-speech.org/archive/SSW_2019/pdfs/SSW10_P_3-3.pdf">paper.pdf</a>
                            <a class="btn" href="https://docs.google.com/presentation/d/1-Xm0MyIj0TP128MuH_CUrQPker99M0EOueJSwYNJOPg/edit?usp=sharing">slides.google</a>
                            <a class="btn" href="documents/SSW19_poster.pdf">poster.pdf</a>
                        </li>

                        <li id="interspeech_2019">
                            Jason Fong, Pilar Oplustil Gallegos, <b>Zack Hodari</b>, Simon King, (2019) <cite>Investigating the robustness of sequence-to-sequence text-to-speech models to imperfectly-transcribed training data.</cite> In Proceedings of Interspeech, Graz, Austria, 2019, pp. 1546-1550.</br>
                            <a class="btn" href="https://www.isca-speech.org/archive/Interspeech_2019/abstracts/1824.html">bib.html</a>
                            <a class="btn" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1824.pdf">paper.pdf</a>
                            <a class="btn" href="documents/IS19_poster.pdf">poster.pdf</a>
                        </li>

                        <li id="interspeech_2018">
                            <b>Zack Hodari</b>, Oliver Watts, Srikanth Ronanki, Simon King, (2018) <cite>Learning interpretable control dimensions for speech synthesis by using external data.</cite> In Proceedings of Interspeech, Hyderabad, India, 2018, pp. 32-36.</br>
                            <a class="btn" href="https://www.isca-speech.org/archive/Interspeech_2018/abstracts/2075.html">bib.html</a>
                            <a class="btn" href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/2075.pdf">paper.pdf</a>
                            <a class="btn" href="documents/IS18_slides.pptx">slides.pptx</a>
                            <a class="btn" href="documents/IS18_poster.pdf">poster.pdf</a>
                        </li>
                        
                        <li id="uk_speech_2017">
                            <b>Zack Hodari</b>, Simon King, (2017) <cite>A learned emotion space for emotion recognition and emotive speech synthesis.</cite> In Proceedings of UK Speech, Cambridge, UK, 2017.</br>
                            <a class="btn" href="http://mi.eng.cam.ac.uk/UKSpeech2017/poster2.html">bib.html</a>
                            <a class="btn" href="documents/UK_Speech_poster.pdf">poster.pdf</a>
                        </li>

                        <li id="mscr_research">
                            <b>Zack Hodari</b> (2017) <cite>A Learned Emotion Space for Emotion Recognition and Emotive Speech Synthesis</cite>. MScR thesis, University of Edinburgh.</br>
                            <b>Summary: </b>My MScR research focussed on learning a description of emotion to improve upon issues with current labelling techniques. In addition, I evaluated the proposed technique using style adaptation for speech synthesis.</br>
                            <a class="btn" href="documents/MScR_thesis_electronic.pdf">thesis.pdf</a>
                            <a class="btn" href="documents/UK_Speech_poster.pdf">poster.pdf</a>
                        </li>

                    </ul>
                </section>


                <section id="personal">
                    <h2>Personal</h2>
                    <p>
                        When I'm not staring at a computer screen, I enjoy board games, hiking, travelling, cooking, baking, woodworking, juggling, and whisky.
                    </p>
                </section>

            </main>

        </div>

        <footer>
            <div class="wrapper">
                <section>
                <div class="wrapper-left">
                    <h2>Contact</h2>
                    <p>Message me on <a class="btn" href="http://linkedin.com/in/zackhodari"> LinkedIn </a></p>
                </div>
                </section>
            </div>
        </footer>

    </body>
</html>
